{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9527395,"sourceType":"datasetVersion","datasetId":5723493}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"color:orange; text-shadow: 1px 1px 2px #000000; font-weight:bold; font-size:30px; text-align:center;\">\n    Warehouse Workload Log Analysis\n</div>\n\n<hr style=\"border:1px solid black;\">\n\nThis analysis focuses on evaluating a **hand-logged dataset** detailing daily warehouse activities, particularly emphasizing workloads related to **batch, inbound, and outbound processes**. Envisioned as a vast dataset, the log will be processed through an **ETL (Extract, Transform, Load) pipeline** to efficiently manage and handle the large volume of data for optimal analysis.\n\nThe goal of the analysis is to **identify trends and patterns** in warehouse operations, providing a deeper understanding of how each workflow—batching, inbound, and outbound—affects overall performance. Key areas of focus will include workload distribution, time variations across tasks, and the interconnectedness of different operations. By uncovering these insights, the analysis aims to inform decisions that can **optimize warehouse efficiency**, improve task coordination, and enhance resource management.\n\nThe insights derived will support **data-driven strategies** to address workload imbalances and improve operational flow, ultimately boosting warehouse productivity.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#000080; color:white; padding:10px; font-size:20px;\">\n    1. Extract Data by Date and Organize into a List of Dictionaries\n</div","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:teal;font-size:16px;font-weight:bold\">\n   1.1 Library Imports\n</div>","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nfrom datetime import datetime\nimport pandas as pd\nfrom pprint import pprint \nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport numpy as np\nimport seaborn as sns\n\n\n# Get the current working directory\ncurrent_path = os.getcwd()\nprint(current_path)\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-10-02T04:56:41.851488Z","iopub.execute_input":"2024-10-02T04:56:41.85267Z","iopub.status.idle":"2024-10-02T04:56:42.899881Z","shell.execute_reply.started":"2024-10-02T04:56:41.852569Z","shell.execute_reply":"2024-10-02T04:56:42.898597Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color:teal;font-size:16px;font-weight:bold\">\n   1.2 Develop Functions for Parsing and Formatting Data\n</div>","metadata":{}},{"cell_type":"markdown","source":"##### 1.2.1 Time Range Formatting Function","metadata":{}},{"cell_type":"code","source":"def parse_time(time_str):\n    \"\"\"Parse and format the time range to ensure two-digit hours and minutes.\"\"\"\n    try:\n        start_time, end_time = time_str.split('-')\n        return f\"{datetime.strptime(start_time.strip(), '%H:%M').strftime('%H:%M')}-\" \\\n               f\"{datetime.strptime(end_time.strip(), '%H:%M').strftime('%H:%M')}\"\n    except ValueError:\n        return time_str  # Return original format if parsing fails","metadata":{"execution":{"iopub.status.busy":"2024-10-02T04:56:42.902351Z","iopub.execute_input":"2024-10-02T04:56:42.903286Z","iopub.status.idle":"2024-10-02T04:56:42.909891Z","shell.execute_reply.started":"2024-10-02T04:56:42.903231Z","shell.execute_reply":"2024-10-02T04:56:42.908769Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### 1.2.2 Label Categorization Function","metadata":{}},{"cell_type":"code","source":"def categorize_label(label):\n    \"\"\"Categorize based on label prefix.\"\"\"\n    #label = re.sub(r'[\\s-]', '_', label.strip().upper())  # Format label for consistency\n    if label.startswith('IB_'):\n        return 'Inbound'\n    elif label.startswith('OB_'):\n        return 'Outbound'\n    return label.title()  # Use label as category if no specific prefix","metadata":{"execution":{"iopub.status.busy":"2024-10-02T04:56:42.911509Z","iopub.execute_input":"2024-10-02T04:56:42.912099Z","iopub.status.idle":"2024-10-02T04:56:42.923394Z","shell.execute_reply.started":"2024-10-02T04:56:42.912046Z","shell.execute_reply":"2024-10-02T04:56:42.9222Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### 1.2.3 Regex-Based Split Function","metadata":{}},{"cell_type":"code","source":"def split_with_regex(item):\n    \"\"\"\n    Applies a regex pattern to extract specific groups from a given string.\n\n    This function uses a predefined regex pattern to match and extract four groups from the input string:\n    - 'label': The main text label before the equal sign ('=').\n    - 'Numbers': A list of integers extracted after the equal sign ('=').\n    - 'Time': A time range in the format 'hh:mm-hh:mm', standardized to two-digit hours and minutes.\n    - 'Notes': Optional descriptive notes starting with an asterisk ('*').\n    - 'Weight': Optional weigth starting with comma or equal sign(',' or '=')\n\n    Parameters:\n    ----------\n    item : str\n        The input string to be parsed by the regex pattern.\n\n    Returns:\n    -------\n    dict or None\n        A dictionary containing the parsed groups if the input matches the pattern, with keys:\n        - 'Label': str, the main label extracted from the string.\n        - 'Numbers': list of int, the cleaned numbers extracted from the string.\n        - 'Time': str, the standardized time range with two-digit hours and minutes.\n        - 'Notes': str, any additional notes extracted from the string.\n        - 'Weight': int, extract from the string.\n        - 'Category': str, extract from Label based on the prefix\n        Returns None if the input does not match the regex pattern.\n    \"\"\"\n    match_obj = pattern.match(item)\n    if not match_obj:\n        return None\n\n    label = re.sub(r'[\\s*-]', '_', match_obj.group(1).strip().upper())  # Formatted Label, extracted from the string\n    category = categorize_label(label)  # Categorize the label\n    numbers = [int(num) for num in re.findall(r'\\d+', match_obj.group(2))]\n    time = parse_time(match_obj.group(3)) if match_obj.group(3) else None\n    notes = match_obj.group(4).strip() if match_obj.group(4) else None\n    weight = int(match_obj.group(5)) if match_obj.group(5) else None  # Convert weight to integer\n\n    return {\n        'Label': label,\n        'Category': category,\n        'Numbers': numbers,\n        'Time': time,\n        'Notes': notes,\n        'Weight': weight\n    }","metadata":{"execution":{"iopub.status.busy":"2024-10-02T04:56:42.924605Z","iopub.execute_input":"2024-10-02T04:56:42.924964Z","iopub.status.idle":"2024-10-02T04:56:42.935983Z","shell.execute_reply.started":"2024-10-02T04:56:42.924903Z","shell.execute_reply":"2024-10-02T04:56:42.934926Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### 1.2.4 Parse Data Line-by-Line into a List of Dictionaries","metadata":{}},{"cell_type":"code","source":"# File path\nfile_path = r\"/kaggle/input/grocery-warehouse-task-log-dataset/gw_task_log.txt\"\n\n# Initialize list to store parsed data as dictionaries\nparsed_data = []\n\n# Define a regex pattern to identify date lines (case-insensitive)\ndate_pattern = re.compile(r'^[A-Za-z]+\\s*\\d{1,2}\\s*$', re.IGNORECASE)\n\n# Define the regex pattern\npattern = re.compile(\n    r'([A-Za-z\\s_-]+)\\s*=\\s*([\\d+\\s+]+)'  # Category and Numbers\n    r'(?:\\s*@\\s*(\\d{1,2}:\\d{1,2}\\s*-\\s*\\d{1,2}:\\d{1,2}))?'  # Optional Time Range\n    r'(?:\\s*\\*([^=]*))?'  # Optional Note, non-greedy, up to a comma\n    r'(?:[=]\\s*(\\d+)?\\s*[lL][bB][sS])?' \n)\n\n# Read the file line-by-line and parse data by date intervals\nwith open(file_path, \"r\") as file:\n    current_date = \"\"\n    for line in file:\n        line = line.strip()\n        # Check if the line matches a date pattern\n        if date_pattern.match(line):\n            current_date = line.title()  # Set the current date with proper capitalization\n        elif current_date:\n            # Parse the line into components using split_with_regex\n            parsed_entry = split_with_regex(line)\n            if parsed_entry:  # Check if parsed_entry is not None before proceeding\n                parsed_entry['Date'] = current_date + ', 2024'  # Add the current date to the parsed entry\n                parsed_data.append(parsed_entry)  # Append only valid entries\n\n# Display the parsed data in a readable format\npprint(parsed_data)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-02T04:56:42.939046Z","iopub.execute_input":"2024-10-02T04:56:42.93947Z","iopub.status.idle":"2024-10-02T04:56:43.031103Z","shell.execute_reply.started":"2024-10-02T04:56:42.939429Z","shell.execute_reply":"2024-10-02T04:56:43.030016Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color:teal;font-size:16px;font-weight:bold\">\n   1.3 Convert to DataFrame and Perform Post-Cleanup\n</div>","metadata":{}},{"cell_type":"markdown","source":"##### 1.3.1 DataFrame Conversion and Verification","metadata":{}},{"cell_type":"code","source":"# Convert the list of dictionaries into a DataFrame for easier data manipulation\ndf = pd.DataFrame(parsed_data)\nprint(df.info())\n# Display the last 10 rows of the DataFrame to verify the calculations and the dataframe's final state\nprint (df.tail(3))","metadata":{"execution":{"iopub.status.busy":"2024-10-02T04:56:43.032554Z","iopub.execute_input":"2024-10-02T04:56:43.033032Z","iopub.status.idle":"2024-10-02T04:56:43.05514Z","shell.execute_reply.started":"2024-10-02T04:56:43.032978Z","shell.execute_reply":"2024-10-02T04:56:43.053997Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### 1.3.2 Column Addition, Modification, and Data Type Adjustment","metadata":{}},{"cell_type":"code","source":"# Convert column names to captialize e using .str.title()\ndf.columns = df.columns.str.title()\n\n# Calculate the sum of the list of integers in each row of the 'Numbers' column and store it in a new column 'Num_Sum'\ndf['Num_Sum'] = df['Numbers'].apply(sum)\n\n# Calculate the total count of integers in each list within the 'Numbers' column and store it in a new column 'Num_Count'\ndf['Num_Count'] = df['Numbers'].apply(len)\n\n# Convert 'date' column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\ndf.tail()","metadata":{"execution":{"iopub.status.busy":"2024-10-02T04:56:43.056676Z","iopub.execute_input":"2024-10-02T04:56:43.057056Z","iopub.status.idle":"2024-10-02T04:56:43.087142Z","shell.execute_reply.started":"2024-10-02T04:56:43.057016Z","shell.execute_reply":"2024-10-02T04:56:43.085974Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#000080; color:white; padding:10px; font-size:20px;\">\n    2. Exploratory Data Analysis (EDA)\n</div","metadata":{}},{"cell_type":"code","source":"# Subset the data\ndf_sub = df[df['Category'].isin(['Inbound', 'Outbound', 'Batch'])]\n\n# Group by 'Date' and 'Category' and aggregate the sum of 'Num_Sum' and 'Weight'\ndf_grouped = df_sub.groupby(['Date', 'Category'])[['Num_Sum', 'Weight']].sum().reset_index()\ndf_grouped.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T04:56:43.088964Z","iopub.execute_input":"2024-10-02T04:56:43.089414Z","iopub.status.idle":"2024-10-02T04:56:43.109562Z","shell.execute_reply.started":"2024-10-02T04:56:43.089362Z","shell.execute_reply":"2024-10-02T04:56:43.108371Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a box plot for 'Num_Sum' across the 'Category'\nplt.figure(figsize=(10, 4))\nsns.boxplot(x='Category', y='Num_Sum', data=df_grouped)\nplt.title('Daily Volume by Category')\n\n# Add a custom Y-axis label\nplt.ylabel('Volume')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-02T04:56:43.111161Z","iopub.execute_input":"2024-10-02T04:56:43.111535Z","iopub.status.idle":"2024-10-02T04:56:43.501055Z","shell.execute_reply.started":"2024-10-02T04:56:43.111484Z","shell.execute_reply":"2024-10-02T04:56:43.50004Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#000080; color:white; padding:10px; font-size:20px;\">\n    3. Visualize Volume and Weight by Date and Category\n</div","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:teal;font-size:16px;font-weight:bold\">\n   3.1 Preparation for Subplot 1\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"##### 3.1.1 Assign to a New Variable","metadata":{}},{"cell_type":"code","source":"# Re-Assign to new variable\nresult_df = df_grouped\nresult_df.tail(3)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T04:56:43.502432Z","iopub.execute_input":"2024-10-02T04:56:43.502797Z","iopub.status.idle":"2024-10-02T04:56:43.51577Z","shell.execute_reply.started":"2024-10-02T04:56:43.502756Z","shell.execute_reply":"2024-10-02T04:56:43.514512Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### 3.1.2 Create a Pivot Table to Display Volume and Weight by Date and Category","metadata":{}},{"cell_type":"code","source":"# Pivot the table to show num_sum and weight by date and category\npivot_table = result_df.pivot(index='Date', columns='Category', values=['Num_Sum', 'Weight'])\n\n# Display the pivoted DataFrame\npivot_table.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T04:56:43.517579Z","iopub.execute_input":"2024-10-02T04:56:43.518044Z","iopub.status.idle":"2024-10-02T04:56:43.544772Z","shell.execute_reply.started":"2024-10-02T04:56:43.517996Z","shell.execute_reply":"2024-10-02T04:56:43.543616Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color:teal;font-size:16px;font-weight:bold\">\n   3.2 Preparation for Subplot 2: Adding Duration: Total Work Hours\n</div>","metadata":{}},{"cell_type":"code","source":"# Define the regex pattern\n#time_pattern = r'[e|s]os\\s*\\d{1,2}:\\d{1,2}\\s*'\ntime_pattern = r'(eos|sos)\\s*\\d{1,2}:\\d{1,2}\\s*'\n\n# Create df_time by directly from subbetting DFl\ndf_time = df[['Date', 'Notes']].copy()\n\n# Use regex to extract the service times\ndf_time['Service'] = df_time['Notes'].apply(\n    lambda x: re.search(time_pattern, str(x).lower()).group(0) if pd.notnull(x) and re.search(time_pattern, str(x).lower()) else None)\n\n# Filter rows where 'Service' is not null\ndf_time = df_time[['Date', 'Service']][df_time['Service'].notnull()]\n\n\n# Split 'Service' into 'Type' (sos or eos) and 'Clock' (time)\ndf_time[['Type', 'Clock']] = df_time['Service'].str.extract(r'(eos|sos)\\s*(\\d{1,2}:\\d{1,2})')\n\n\n# Pivot the table\npivot_df_time = df_time.pivot_table(index='Date', columns='Type', values='Clock', aggfunc='first').reset_index()\n\n# Combine 'Date' with 'sos' and 'eos' to create full datetime columns\npivot_df_time['sos_full'] = pd.to_datetime(pivot_df_time['Date'].astype(str) + ' ' + pivot_df_time['sos'].astype(str))\npivot_df_time['eos_full'] = pd.to_datetime(pivot_df_time['Date'].astype(str) + ' ' + pivot_df_time['eos'].astype(str))\n\n# Calculate duration in hours\npivot_df_time['Duration'] = ((pivot_df_time['eos_full'] - pivot_df_time['sos_full']).dt.total_seconds() / 3600).round(2)\n\n# Subtract 0.5 hours if duration exceeds 6 hours\npivot_df_time['Duration'] = np.where(pivot_df_time['Duration'] > 6, pivot_df_time['Duration'] - 0.5, pivot_df_time['Duration'])\n\n# Display the final DataFrame\npivot_df_time[['Date', 'sos', 'eos', 'Duration']].head(2)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T04:56:43.546187Z","iopub.execute_input":"2024-10-02T04:56:43.546547Z","iopub.status.idle":"2024-10-02T04:56:43.590872Z","shell.execute_reply.started":"2024-10-02T04:56:43.546508Z","shell.execute_reply":"2024-10-02T04:56:43.589641Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color:teal;font-size:16px;font-weight:bold\">\n   3.3 Plotting\n</div>","metadata":{"execution":{"iopub.status.busy":"2024-10-01T22:53:04.903157Z","iopub.execute_input":"2024-10-01T22:53:04.903637Z","iopub.status.idle":"2024-10-01T22:53:04.911457Z","shell.execute_reply.started":"2024-10-01T22:53:04.903594Z","shell.execute_reply":"2024-10-01T22:53:04.909877Z"}}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Create a figure with 2 subplots, adjusting height ratios\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6), gridspec_kw={'height_ratios': [3, 1]})\n\n# Plot 1: num_sum for Batch, Inbound, and Outbound on the primary y-axis (ax1)\npivot_table['Num_Sum'][['Batch', 'Inbound', 'Outbound']].plot(kind='line', ax=ax1, marker='o')\nax1.set_xlabel('Date')\nax1.set_ylabel('Volume')\nax1.set_title('Volume and Weight by Date by Category', color='teal')\nax1.grid(True)\n\n# Set x-ticks and labels using the actual date index\nax1.set_xticks(pivot_table.index)\nax1.set_xticklabels(pivot_table.index.strftime('%Y-%m-%d'), fontsize=10, rotation=45, ha='right')\n#ax1.set_xticklabels([])\n\n# Plot 2:  weight for Inbound on the secondary y-axis (ax1)\nax1_2 = ax1.twinx()\npivot_table['Weight']['Inbound'].plot(kind='line', ax=ax1_2, marker='x', color='red')\nax1_2.set_ylabel('Weight (lb)')\n\n# Add legends\nax1.legend(loc='upper left', fontsize=8, title_fontsize=9, title='Volume by Category')\nax1_2.legend(['Weight (Inbound)'], loc='upper center', fontsize='small')\n\n# Plot Duration over Date on the second subplot (ax2)\nax2.plot(pivot_df_time['Date'], pivot_df_time['Duration'], marker='o', color='brown')\nax2.set_title('Total Work Hours by Date', color='teal')\n#ax2.set_xlabel('Date')\nax2.set_ylabel('Duration (hr)')\nax2.set_xticks(pivot_df_time['Date'])\n#ax2.set_xticklabels(pivot_df_time['Date'].dt.strftime('%Y-%m-%d'), fontsize = 10, rotation=45)\nax2.set_xticklabels([])\nax2.grid(True)\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\n\n# Show the plots\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-02T04:56:43.592455Z","iopub.execute_input":"2024-10-02T04:56:43.592896Z","iopub.status.idle":"2024-10-02T04:56:44.555112Z","shell.execute_reply.started":"2024-10-02T04:56:43.592854Z","shell.execute_reply":"2024-10-02T04:56:44.553836Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#000080; color:white; padding:10px; font-size:20px;\">\n    4. Analysis of Trends and Patterns in Warehouse Workload\n</div\n    \n1. **Weekly Seasonal Pattern**:\n   - The data reveals a **weekly trend** where the workload is more evenly distributed on **Thursdays**, indicating a balance between inbound and outbound processes, as well as batching operations. However, there is a noticeable **spike in inbound activity on Fridays**, leading to heavier workloads for incoming goods. This pattern suggests that Fridays require more resources or attention for inbound operations, while Thursdays represent a more stable equilibrium across all processes.\n\n2. **Impact of Balanced Workload**:\n   - When the workload is **more balanced across different operations**, it results in a **decrease in the overall volume** being processed. This suggests that when batch, inbound, and outbound processes are evenly spread, the workload is more manageable, which might slow down the pace of operations but allows for a more steady and sustainable workflow. This balance could lead to fewer bottlenecks and reduce the pressure on specific areas, ultimately contributing to improved efficiency across the warehouse..","metadata":{}}]}